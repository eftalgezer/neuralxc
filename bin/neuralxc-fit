#!/usr/bin/python3
from neuralxc.symmetrizer import symmetrizer_factory
from neuralxc.formatter import atomic_shape, system_shape, SpeciesGrouper
from neuralxc.ml.transformer import GroupedPCA, GroupedVarianceThreshold
from neuralxc.ml.transformer import GroupedStandardScaler
from neuralxc.ml import NetworkEstimator as NetworkWrapper
from neuralxc.ml import NXCPipeline
from neuralxc.ml.network import load_pipeline
from neuralxc.ml.utils import SampleSelector, load_data
import sys
import h5py
import json
import numpy as np
from sklearn.model_selection import GridSearchCV
import pandas as pd
import argparse

def match_hyperparameter(hp, parameters):
    matches = []
    for par in parameters:
        if hp in par:
            matches.append(par)
    if len(matches) != 1:
        raise ValueError('{} matches found for hyperparameter name. Must be exactly 1'.format(len(matches)))
    return matches[0]

def to_full_hyperparameters(hp, parameters):
    full = {}
    for name in hp:
        full[match_hyperparameter(name, parameters)] = hp[name]
        if not isinstance(full[match_hyperparameter(name, parameters)], list):
            full[match_hyperparameter(name, parameters)] = [full[match_hyperparameter(name, parameters)]]
    return full


def get_default_pipeline(basis, species, symmetrizer_type= 'casimir', pca_threshold = 0.999):

    symmetrizer_instructions = {'basis': basis,
                         'symmetrizer_type': symmetrizer_type}

    spec_group = SpeciesGrouper(basis, species)
    symmetrizer = symmetrizer_factory(symmetrizer_instructions)
    var_selector = GroupedVarianceThreshold()

    estimator = NetworkWrapper(1, 4, 0,
                            alpha=0.001, max_steps = 4001, test_size = 0.0,
                               valid_size=0, random_seed = None)

    pipeline_list = [('spec_group',  spec_group),
                     ('symmetrizer', symmetrizer),
                     ('var_selector', var_selector)]

    if pca_threshold < 1:
        pca = GroupedPCA(n_components= pca_threshold, svd_solver='full')
        pipeline_list.append(('pca', pca))

    pipeline_list.append(('scaler', GroupedStandardScaler()))
    pipeline_list.append(('estimator', estimator))

    basis_instructions = basis
    symmetrizer_instructions = {'symmetrizer_type': symmetrizer_type}

    return  NXCPipeline(pipeline_list, basis_instructions=basis_instructions,
                             symmetrize_instructions=symmetrizer_instructions)

def load_sets(datafile, baseline, reference):

    all_species = []
    X, y, basis, species =\
        load_data(datafile, baseline, reference, 0 ,False)

    X = np.concatenate([np.array([0]*len(X)).reshape(-1,1), X],axis = 1)

    data = np.concatenate([X,y.reshape(-1,1)], axis =1)
    return data, basis, [species]


if __name__ == '__main__':

    parser = argparse.ArgumentParser(description='Fit a NeuralXC model')
    parser.add_argument('data', action='store', help ='Path to hdf5 file containing dataset')
    parser.add_argument('config', action='store', help='Path to .json configuration file setting hyperparameters')
    parser.add_argument('-mask', action=('store_true'), help='Create a config file mask')

    args = parser.parse_args()
    datapath = args.data
    inputfile = args.config

    inp = json.loads(open(inputfile,'r').read())
    datafile = h5py.File(datapath,'r')

    if (not 'baseline' in inp) or (not 'reference' in inp):
        raise ValueError('Must specify sources for baseline and reference data')

    data, basis, species = load_sets(datafile, inp['baseline'], inp['reference'])
    datafile.close()

    pipeline = get_default_pipeline(basis, species)

    if args.mask:
        params ={key: value for key, value in pipeline.start_at(2).get_params().items() if '__' in key}
        inp.update({'hyperparameters': params})
        inp.update({'cv':2,'n_jobs':1})
        open('mask.json','w').write(json.dumps(inp, indent = 4))
    else:
        if 'hyperparameters' in inp:
            hyper = inp['hyperparameters']
        else:
            print('No hyperparameters specified, fitting default pipeline to data')
            pipeline.fit(data)
            sys.exit()

        hyper = to_full_hyperparameters(hyper , pipeline.get_params())

        cv = inp.get('cv',2)
        n_jobs = inp.get('n_jobs', 1)
        verbose = inp.get('verbose',10)

        print(hyper)
        best_model = GridSearchCV(pipeline, hyper, cv=cv, n_jobs= n_jobs, refit=True, verbose = verbose)
        best_model.fit(data)


        open('best_params.json','w').write(json.dumps(best_model.best_params_))
        pd.DataFrame(best_model.cv_results_).to_csv('cv_results.csv')
        best_model.best_estimator_.start_at(2).save('best_model',True)
