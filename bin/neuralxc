#!/usr/bin/python3
import argparse
from neuralxc.drivers import *
if __name__ == '__main__':

    parser = argparse.ArgumentParser(description='Add data to hdf5 file')
    subparser = parser.add_subparsers()

    #================ Plot Basis Set ================
    basis = subparser.add_parser('basis', description='Plot radial basis functions')
    basis.add_argument(
        'basis', action='store', type=str, help='Path to .json file \
        containing the basis to plot')
    basis.set_defaults(func=plot_basis)

    # =======================================================
    # =============== Data routines =========================
    # =======================================================
    dat = subparser.add_parser('data', description='Routines to manipulate datasets')
    datsub = dat.add_subparsers()

    #================ Add data ================
    adddat = datsub.add_parser('add', description='Add data to hdf5 file')
    adddat.add_argument('hdf5', action='store', type=str, help='Path to hdf5 file')
    adddat.add_argument('system', action='store', type=str, help='System')
    adddat.add_argument('method', action='store', type=str, help='Method')
    adddat.add_argument(
        'add', action='store', type=str, nargs='*', help='Which quantities to add (energy, forces, density)')
    adddat.add_argument('-traj', metavar='traj', type=str, default='', help='Path to .xyz/.traj file')
    adddat.add_argument('-density', metavar='density', type=str, default='', help='Path to basis representation file')
    adddat.add_argument('-override', action=('store_true'), help='If exists, override?')
    adddat.add_argument('-slice', metavar='slice', type=str, default=':', help='Only add slice of dataset')
    adddat.add_argument(
        '-zero',
        metavar='zero',
        type=int,
        default=None,
        help='Shift energies by this value, if not set, use minimum of dataset')
    adddat.set_defaults(func=add_data_driver)

    def inspectdat_driver(args):
        subprocess.Popen('h5dump -n ' + args.hdf5, shell=True)

    #================ Inspect data ================
    inspectdat = datsub.add_parser('inspect', description='Inspect data in hdf5 file')
    inspectdat.add_argument('hdf5', action='store', type=str, help='Path to hdf5 file')
    inspectdat.set_defaults(func=inspectdat_driver)

    #================ Split data ================
    splitdat = datsub.add_parser('split', description='Split dataset (e.g. into training and test set)')
    splitdat.add_argument('hdf5', action='store', type=str, help='Path to hdf5 file')
    splitdat.add_argument('group', action='store', type=str, help='Which group to apply slicing to')
    splitdat.add_argument('label', action='store', type=str, help='New label for slice')
    splitdat.add_argument('--slice', metavar='slice', default=':', type=str, help='Slice in numpy notation')
    splitdat.add_argument(
        '--comp', metavar='comp', default='', type=str, help='Store complementary slice under this group')
    splitdat.set_defaults(func=split_data_driver)

    #================ Delete data ================
    deldat = datsub.add_parser('delete', description='Delete group inside hdf5 file')
    deldat.add_argument('hdf5', action='store', type=str, help='Path to hdf5 file')
    deldat.add_argument('group', action='store', type=str, help='Which group to delete')
    deldat.set_defaults(func=delete_data_driver)

    #================ Sample data ================
    sampledat = datsub.add_parser(
        'sample', description='Sample the data for a given basis set using KMeans cluster in feature-space')
    sampledat.add_argument('preprocessor', action='store', help='Path to configuration file for preprocessor')
    sampledat.add_argument('size', action='store', type=int, help='Sample size')
    sampledat.add_argument('--dest', action='store', type=str, default='sample.npy', help='Save to')
    sampledat.add_argument('--hdf5', metavar='hdf5', type=str, nargs=2, help='Path to hdf5 file, baseline data')
    sampledat.add_argument('--cutoff', metavar='cutoff', type=float, default=0.0, help='Cut off extreme datapoints')
    sampledat.set_defaults(func=sample_driver)


    #================ Merge datasets =============
    mergedat = datsub.add_parser('merge', description='Merge datasets inside hdf5 file')
    mergedat.add_argument('file', action='store', type=str, help='Path to hdf5 file')
    mergedat.add_argument('--base', action='store', type=str,nargs='+', help='Baseline sets', required = True)
    mergedat.add_argument('--ref', action='store', type=str,nargs='+', help='Reference sets', required = True)
    mergedat.add_argument('--o', action='store', type=str, help='Destination', required = True)
    mergedat.add_argument('--optE0', action='store_true', help='Optimize energy offset across datasets (recommended)')
    mergedat.add_argument('--pre', action='store', default='',type=str, help='Preprocessor file defining basis set')

    mergedat.set_defaults(func=merge_data_driver)


    # =======================================================
    # =============== Model routines ========================
    # =======================================================

    # =============== Fitter =====================

    fit = subparser.add_parser('fit', description='Fit a NeuralXC model')
    fit.add_argument('preprocessor', action='store', help='Path to configuration file for preprocessor')
    fit.add_argument('config', action='store', help='Path to .json configuration file setting hyperparameters')
    fit.add_argument('--mask', action=('store_true'), help='Create a config file mask')
    fit.add_argument(
        '--hdf5', metavar='hdf5', type=str, nargs=3, help='Path to hdf5 file, baseline data, reference data')
    fit.add_argument('-sets', metavar='sets', type=str, default='', help='Path to file defining sets')
    fit.add_argument(
        '--sample', metavar='sample', type=str, default='', help='Only use a subsample of data contained in hdf5 file')
    fit.add_argument('-cutoff', metavar='cutoff', type=float, default=0.0, help='Cut off extreme datapoints')
    fit.add_argument(
        '--model', metavar='model', type=str, default='', help='Continue training model found at this location')
    fit.add_argument(
        '--ensemble',
        action='store_true',
        help='Derive pipeline from -model and only fit final estimator, to be able to build\
    an ensemble of estimators later on')
    fit.add_argument('--hyperopt', action='store_true', help='Do a hyperparameter optimzation')
    fit.add_argument('--b', metavar='b',type=float, default=-1, help='Weight decay parameter (supercedes the one specified in config file)')
    fit.set_defaults(func=fit_driver)

    # =============  Adiabatic  ====================
    ad = subparser.add_parser('adiabatic', description='Fit a NeuralXC model adiabatically')
    ad.add_argument('preprocessor', action='store', help='Path to configuration file for preprocessor')
    ad.add_argument('config', action='store', help='Path to .json configuration file setting hyperparameters')
    ad.add_argument('engine', action='store', type=str, help='Command to start engine')
    ad.add_argument(
        '--data', metavar='data', type=str, default='', help='Start from this dataset instead of computing iteration 0')
    ad.add_argument(
        '--config2',
        metavar='config2',
        type=str,
        default='',
        help='Path to .json configuration file setting hyperparameters used for iteration > 0')
    ad.add_argument('--maxit', metavar='maxit', type=int, default='5', help='Maximum number of iterations')
    ad.add_argument(
        '--tol',
        metavar='tol',
        type=float,
        default='0.0005',
        help='Tolerance in energy defining whether iterative training converged')
    ad.add_argument(
        '--b0',
        metavar='b0',
        type=float,
        default='1',
        help='Initial value for weight decay')
    ad.add_argument(
        '--b_decay',
        metavar='b_decay',
        type=float,
        default='0.1',
        help='Decay for regularization parameter b (applied after every iteration)')
    ad.add_argument(
        '--hotstart',
        metavar='hotstart',
        type=int,
        default='0',
        help='Continue workflow at this iteration ( -1: only do testing )')
    ad.add_argument('-sets', metavar='sets', type=str, default='', help='Path to file defining sets')
    ad.add_argument(
        '--nozero', action='store_true', help='Do not automatically set energy origins for every dataset by using min')
    ad.add_argument(
        '--model0',
        metavar='model0',
        type=str,
        default='',
        help='Build new model on top of model0 as a stacked estimator')
    ad.add_argument(
        '--fullstack', action='store_true', help='If model0 specified do full stack instead of stacking estimators')
    ad.set_defaults(func=adiabatic_driver)

    # ============= Workflow ====================
    wf = subparser.add_parser('workflow', description='Fit a NeuralXC model iteratively')
    wf.add_argument('preprocessor', action='store', help='Path to configuration file for preprocessor')
    wf.add_argument('config', action='store', help='Path to .json configuration file setting hyperparameters')
    wf.add_argument('engine', action='store', type=str, help='Command to start engine')
    wf.add_argument(
        '--data', metavar='data', type=str, default='', help='Start from this dataset instead of computing iteration 0')
    wf.add_argument(
        '--config2',
        metavar='config2',
        type=str,
        default='',
        help='Path to .json configuration file setting hyperparameters used for iteration > 0')
    wf.add_argument('--maxit', metavar='maxit', type=int, default='5', help='Maximum number of iterations')
    wf.add_argument(
        '--tol',
        metavar='maxit',
        type=float,
        default='0.0005',
        help='Tolerance in energy defining whether iterative training converged')
    wf.add_argument(
        '--hotstart',
        metavar='hotstart',
        type=int,
        default='0',
        help='Continue workflow at this iteration ( -1: only do testing )')
    wf.add_argument('-sets', metavar='sets', type=str, default='', help='Path to file defining sets')
    wf.add_argument(
        '--nozero', action='store_true', help='Do not automatically set energy origins for every dataset by using min')
    wf.add_argument(
        '--model0',
        metavar='model0',
        type=str,
        default='',
        help='Build new model on top of model0 as a stacked estimator')
    wf.add_argument(
        '--fullstack', action='store_true', help='If model0 specified do full stack instead of stacking estimators')
    wf.set_defaults(func=workflow_driver)

    # =============== Evaluate =====================

    eval = subparser.add_parser('eval', description='Evaluate a NeuralXC model')
    eval.add_argument('-model', metavar='model', default='', help='Path to NeuralXC model')
    eval.add_argument(
        '-hdf5', metavar='hdf5', type=str, nargs=3, help='Path to hdf5 file, baseline data, reference data')
    eval.add_argument('-plot', action='store_true', help='Create scatterplot?')
    eval.add_argument('-savefig', action='store', type=str, default='', help='Save scatterplot?')
    eval.add_argument('-cutoff', metavar='cutoff', type=float, default=0.0, help='Cut off extreme datapoints')
    eval.set_defaults(predict=False)
    eval.set_defaults(func=eval_driver)

    # =============== Predict =====================

    pred = subparser.add_parser('predict', description='Predict energies with NeuralXC model')
    pred.add_argument('-model', metavar='model', help='Path to NeuralXC model')
    pred.add_argument(
        '-hdf5', metavar='hdf5', type=str, nargs=2, help='Path to hdf5 file, baseline data, reference data')
    pred.add_argument('-dest', metavar='dest', type=str, default='prediction', help='Destination where to store data')
    pred.set_defaults(predict=True)
    pred.set_defaults(func=eval_driver)

    # ============= Stack ================

    ens = subparser.add_parser('stack', description='Stack mutltiple NeuralXC models')
    ens.add_argument(
        '-operation', metavar='operation', type=str, default='sum', help='Operation to combine estimator outputs')
    ens.add_argument('-dest', metavar='dest', type=str, default='stacked_ensemble', help='Model destination')
    ens.add_argument('-estonly', action=('store_true'), help='Only stack the final estimators')
    ens.add_argument('models', action='store', type=str, nargs='+', help='Paths to models to combine')
    ens.set_defaults(func=ensemble_driver)

    # ============= Chain ================

    chain = subparser.add_parser('chain', description='Chain NeuralXC models')
    chain.add_argument('config', action='store', help='Path to .json configuration file setting hyperparameters')
    chain.add_argument('-model', metavar='model', type=str, help='Continue training model found at this location')
    chain.add_argument('-dest', metavar='dest', type=str, default='chained_estimator', help='Model destination')
    chain.set_defaults(func=chain_driver)

    #================ Merge ==========

    merge = subparser.add_parser('merge', description='Merges a chained NumpyNetworkEstimator into one model')
    merge.add_argument('chained', action='store', help='Path to chained model')
    merge.add_argument('merged', action='store', help='Destination for numpy model')
    merge.set_defaults(func=merge_driver)

    #================ Tensorflow model converter ==========

    tfcon = subparser.add_parser('convert-tf', description='Converts a tensorflow NeuralXC into a numpy NeuralXC')
    tfcon.add_argument('tf', action='store', help='Path to tensorflow model')
    tfcon.add_argument('np', action='store', help='Destination for numpy model')
    tfcon.set_defaults(func=convert_tf)

    # =======================================================
    # =============== Preprocessor ========================
    # =======================================================
    pre = subparser.add_parser('pre', description='Preprocess electron density')
    pre.add_argument('preprocessor', action='store', help='Path to configuration file for preprocessor')
    pre.add_argument(
        '-dest',
        metavar='dest',
        type=str,
        default='.tmp/',
        help='Destination where to store data,\
                                                                        can be either a directory or an .hdf5 file (with groups)'
    )
    pre.add_argument('-mask', action=('store_true'), help='Create a config file mask')
    pre.add_argument('-xyz', metavar='xyz', type=str, default='', help='Path to xyz file')
    pre.set_defaults(func=pre_driver)

    args = parser.parse_args()

    args.func(args)
